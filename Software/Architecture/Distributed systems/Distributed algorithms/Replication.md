---
date: 2023-08-08
---
# Replication

Репликация - это пример [[Horizontal scaling]] и ее суть проста - хранение нескольких копий данных на разных нодах кластера.

Это позволяет:

1. повысить [[Availability]]: в случае недоступности/падения одной из нод чтение можно производить с других реплик
1. повысить [[Scalability]] чтения: нагрузку на чтение можно распределять по всем репликам
1. снизить [[Latency]]: читать данные с той реплики, которая физически ближе к клиенту

Репликация отлично работает, если данные после записи не обновляются, например *Content Delivery Network*. Но в случае, если есть обновления, возникает проблемы с согласованностью данных. И тут имеем классический trade-off: чем больше хотим доступности и масштабируемости, тем больше должны ослаблять требования согласованности.

## Виды репликации [[Async vs sync]]

1. синхронная - запись считается успешной только если в ходе того же запроса от пользователя данные были реплицированы
1. асинхронная - производится в фоне, после того как на запрос пользователя ответили успехом. Неизбежны проблемы с отставанием реплик - *replication lag*. Следствием этого будут возникать проблемы с согласованностью данных.

## Способы репликации

1. statement replication. Для реплицирования данных передаются прямо запросы, которые были выполнены на лидере(или той реплике, на которую попала запись в случае *leaderless*). Достоинства: простота. Недостаток: запросы могут быть завязаны на состояние или контекст исполнения(dateTime.Now, autoincrement), могут инициировать работу триггеров или хранимых процедур. Короче говоря результат выполнения этого запроса на репликах будет иной.
1. передача *write ahead log (WAL)*. Достоинство - простота(СУБД в любом случае пишет WAL). Недостаток: формат WAL является низкоуровневой деталью реализации СУБД и может меняться от версии к версии, не говоря уже о совместимости WAL для разных СУБД(на случай если данные реплицируются в другую СУБД). Т.о. на фоловере должно хранилище совпадать вплоть до версии, что может делать невозможным обновление СУБД без полной остановки кластера.
1. логическая репликация. Идея похожая на передачу WAL - передается что в итоге было сделано, но только операции описываются на более высоком уровне абстракции и не привязаны к конкретному формату WAL. Это отвязывает фоловеров от лидера.

## Реализация

### Single leader

Среди нод кластера выбирают *leader*, остальные считаются *followers*.

Все записи идут на лидера, после чего(*синхронно или асинхронно*) реплицируются на фоловеров.

Отказы нод:

1. фоловер
    1. поднимается новая нода
    1. новый фоловер "догоняет" по данным остальных
    1. новый фоловер включается в работу кластера
1. лидер. Восстановление после отказа лидера - это более сложный процесс, который называется *failover*. В случае асинхронной репликации есть риск потери данных. В рамках failover'a проиводится [[Leader election]].

Single-leader является наиболее распространенной схемой репликации, особенно в реляционных СУБД. Пропускание записей через лидера делает репликацию более простой для понимания и делает нерелевантными проблемы *concurrent* записей. Но лидер в данной схеме является узким горлышком с точки зрения масштабируемости записи и отказоустойчивости.

### Multi leader

В кластере существует несколько лидеров. У каждого из лидеров есть свои фоловеры, плюс происходит синхронизация лидеров между собой.

Где актуально:

1. наличие нескольких географияески распределенных дата центров. В каждом дата центре есть свой лидер и его фоловеры. Запросы пользователей конкретных регионов идут в конкретные датацентры, а значит на конкретные лидеры.
1. приложения с офлайн доступом. Любой мобильное приложение может иметь свою СУБД, а значит выступать в роли лидера, который может модифицировать данные и потом реплицировать их в облако.
1. приложения одновременного редактирования документов, например Google Docs. Каждый редактор является лидером и правит свою версию документа, из которых потом должно сложиться итоговое состояние.

Главная проблема Multi-leader replication - это **конфликты**, которые неизбежно возникнут при одновременном изменении одних и тех же данных на разных лидерах.

### Leaderless

---

## Источники

1. [[Designing Data-Intensive Applications book ]]. Chapter 5.

## Ссылки

1. [[Consistency models]]
