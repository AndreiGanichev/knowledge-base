---
date: 2022-07-04
---

**Liveness:** лидер в итоге выбран
**Safety**: выбран только один лидер (отсутствует *split brain*)

### Достоинства

Наличие лидера/координатора упрощает распределенные алгоритмы и делает их более оптимальными: снижает количество передаваемых сообщений.

### Недостатки

Лидер является узким местом в системе, а значит снижается отказоустойчивость.
Чтобы нивилировать этот недостаток используют подходы:
1. разные лидеры на разные операции
1. при использовании партиционирования лидер может быть на партицию

### Функции лидера

1. координация распределенного алгоритма
1. координация восстановления/перестройки системы после падения, при инициализации или крупного изменения состояния системы
1. хранение глобального состояния

### Примеры

При партиционировании в *Kafka* используется лидер на партицию. На одной ноде могут располагаться несколько лидеров. Поэтому при падении одной ноды приходится запускать несколько выборов(для каждой партиции). Чтобы не дублировать  одни и те же шаги назначается координатор(лидер) этого процесса. Т.е. выбирается лидер для выборов лидеров партиций. [Kafka documentation](https://kafka.apache.org/documentation/#design_replicamanagment)

Наличие лидера упрощает реализацию алгоритмов, реализующих [[Total order broadcast]]. В этом случае на лидер идут все запросы клиентов и он определяет порядок операций записи. Дальше записи реплицируются на другие ноды, сохраняя этот порядок.

Реализации *RAFT* и *Multi-Paxos* основываются на наличии временного лидера.

### Связь с другими алгоритмами

1. Leader election алгоритм тесно связан с [[Failure detection]] алгоритмом: чтобы выбрать нового лидера нужно сначала узнать, что старый отказал. Кроме этого каждому процессу-фоловеру необходимо понимать об актуальности его данных о текущем лидере.
1. Leader election по смыслу связан с [[Consensus]] алгоритмом: чтобы выбрать лидера нужно, чтобы все процессы кластера пришли к консенсусу насчет того, кто в данный момент является лидером в системе.
1. Может сложиться впечатление, что быть лидером аналогично удержанию распределенной блокировки на какой-то ресурс([[Distributed lock]]). И та, и другая роль временная, и предполагают определенные эксклюзивные права. Но эти алгоритмы все же отличаются:
    1. для системы выгодно, чтобы лидер был один и тот же как можно дольше. Чем реже меняется лидер, тем меньше ресурсов тратиться на выборы. А в случае блокировки наоборот - чем меньше время владения ей, тем лучше, потому что возможно другие процессы ждут возможности ее взять.
    1. в случае наличия лидера процессы обычно знают о том, какой именно процесс выполняет эту роль. В случае распределенной блокировки процессам не обязательно знать кто именно удерживает блокировку, а достаточно знать когда она освободилась.

### Характеристики реализаций алгоритма

1. Вероятность получения split brain. Чтобы полностью избежать spilt brain необходимо при выборах получить голоса от большинства процессов кластера. Тут имеем компромисс между *liveness* и *safety* алгоритма: для обеспечения liveness лидер должен быть выбран и как можно скорее. При этом можно пожертвовать safety и допустить split brain ради производительности выборов. Нужно просто уметь обнаруживать эту ситуацию и разрешать возможные конфликты. Так делают *Raft, Multi Paxos* [[Consensus]].
1. Кол-во процессов, о которых должен знать процесс, который обнаружил отказ лидера
1. Кол-во сообщений, треюбующихся для выбора

### Примеры реализаций

1. *Bully(monarchical)*. Процессы системы ранжируются и очередным лидером становится "живой" процесс с наивысшим рангом. Процесс, который заметил отказ лидера отправляет пинг всем процессам с более высоким рангом. Дальше процесс выбирает из тех процессов, что ответили, процесс с наивысшим рангом и сообщает ему, что он становится лидером. Новоиспеченный лидер после этого сообщает всем остальным процессам о своем избрании.
В случае *network partition* легко получается split brain. Каждый процесс фактически должен знать обо всех других участниках. Другой проблемой является то, что приоритет всегда отдается процессам с более высоким рангом. Если такие процессы оказываются нестабильными и часто отказывают, то leader election будет происходить часто.

1. *Next-in-line follower*. Является вариацией bully алгоритма. И отличается только тем, что действующий лидер явно предоставляет список своих замен. Тогда процесс, обнаруживший отказ будет отправлять пинг только процессу из этого списка, имеющему наивысший ранг. В итоге, если этот процесс "живой", то выборы происходят быстрее и требуют меньшег кол-ва сообщений чем исходный bully алгоритм.


1. *Candidate/ordinariy*. Еще одна попытка сократить количество сообщений, необходимых для выборов. Все процессы делятся на две группы и только candidates могут быть избраны в лидеры. Лидером выбирается кандидат с наивысшим рангом.


Чтобы избежать ситуации, когда **несколько процессов обнаруживают отказ лидера и одновременно запукают выборы**, может применяться *tiebreaker variable*. Это задержка по времени, которую ожидает процесс, обнаруживший отказ лидера, прежде чем инициировать выборы. Величина переменной значительно различается от процесса к процессу, что приводит к тому, что в случае конкурренции процесс скорее всего узнает о начале выборов прежде чам сам сможет их начать.

---

### Источники:
1. Database Internals. Petrov. Oreily, 2019. Chapter 10. Leader election

### Ссылки:
1. [[Distributed algorithms]]
1. [[Kafka design]]