---
date: 2025-06-15
---
# History

ИИ появился на стыке нескольких дисциплин:

1. Нейрофизиология

- модель искусственного нейрона и нейронной сети

2. Математика

- правила формальной логики
- статичтические методы
- теория вероятностей
- теория принятия решений

3. Информатика

- языки программирования
- теория вычислительной сложности
- высокопроизводительные компьютеры

4. Кибернетика

- теория управления сложных систем

Термин ИИ был впервые использован в 1956г на Дартмутском семинаре, который дал точкок в развитииновой дисциплины.

ИИ стал развиваться в трех направлениях:

1. символьный подход - задача решается через действия над понятными человеку символическими обозначениями
1. логическое программирование - задача решается методами формальной логики
1. коннекционизм - задача решается сетью связанных между собой простых элементов

В разное время популярным становилось то или иное направления в зависимости от успехов в нем.

## Рассуждение как поиск

Модели Logic theorist (символьный подход) 1956г и Advice Taker (логическое программирование) 1958г использовали концепцию *рассуждение как поиск*:

> Logic theorist выполняет поиск по дереву для доказательства гипотезы. Корень дерева соответствует известным фактам. Исходящие из корня ветви - это логические операции над фактами. Каждая ветвь приводит к определенному выводу. Один из этих выводов является целью рассуждения. Именно его ищет система.

Поиск являлся настолько важной частью, что в процессе создания Advice Taker был разработан декларативный язык программирования *Lisp* (LISt Processing), который был построен на операциях над списками.

## Коннекционизм

В это направление большой вклад внесли ученые нейрофизики, нейропсихологи. Математический аппарат статистической физики был использован для обучения нейронных сетей(сеть Хопфилда).

### Бинарный пороговый нейрон

Имеет два состояния: возбужденное и невозбужденное. На его вход подаются сигналы от других нейронов. Эти сигналы обрабатывает функция активации. Результат функции передается на выход нейрона. Функция активации бинарного порогового нейрона называется ступенчатой: может принимать два значения 0 и 1. Принимает значение 1, когда сумма входных значений превышает определенный порог.

> ..чтобы решать реальные задачи с помощью нейронноей сети, нужны были средства управления. Для универсальных компьютеров такими средствами стали программы. Это решение не подходило для нейронных сетей. Для них нужен был **принципиально иной подход**.

### Правило Хебба

Дональд Хебб, изучая процессы обучения и памяти в человеческом мозге, сформулировал правило: *Если за активацией нейрона А последовательно следуется активация нейрона Б, связь между ними усиливается. С другой стороны, если за активацией нейрона А не следует активация нейрона Б, связь ослабевает.*

Правило Хебба заложило основу теории обучения искуственных нейронных сетей.

### Перцептрон

[[Perceptron]]

### Методы обратного распространения ошибки

Термин ввел Фрэнк Розенблат в 1962г. Суть обучения сети сотояла в подборе весов. Метод обратного распространения ошибки позволял автоматически обучать сеть. Суть его в том, что после прямого прохода на обучающем примере определяется ошибка вычисления и запускается обратный проход. В процессе обратного прохода алгоритм вычисляет вклад каждого веса в общую ошибку вычисления. Далее веса корректируются в направлении, которое минимизирует ошибку вычислений и повторяется прямой проход.
Обучение заказнчивается либо при достижении допустимого уровня ошибки, либо по истечению определенного количества проходов.

## Причины бума ИИ

Действительно были успехи в ИИ, обусловленные несколькими факторами:

1. для обучения нейронных сетей было предложено исопльзовать графические процессоры (GPU). Они хорошо подходят из-за того, что имеют большое кол-во ядер и следовательно обеспечивают большую степень парралелизма. GPU выполняет параллельную обработку больших блоков данных. Моделирование нейронных сетей - это тоже обработка больших блоков данных, которую можно распарралелить.
1. накопился большой объем данных, которые были использованы для обучения моделей
1. были разработаны эффективные алгоритмы *deep learning*
1. стали доступны фреймворки и библиотеки с открытым исходным кодом для работы с нейронными сетями
1. развитие облачных платформ позволило иметь доступ к большим вычислительным ресурсам

## Deep learning

> Собирательное название для семейства методов машинного обучения. Работают со сложными сетями, имеющими несколько скрытых слоев и сотни параметров.

При обучении разные слои модели работают с разным уровнем абстракции. Например, для изображений это:

1. отдельные линии
1. геометрические фигуры
1. реальные объекты

Так пишут [2] про назначение разных Transformer blocks:

> As is common in deep learning, it's hard to say exactly what each of these layers is doing, but we have some general ideas: the earlier layers tend to focus on learning lower-level features and patterns, while the later layers learn to recognize and understand higher-level abstractions and relationships. In the context of natural language processing, the lower layers might learn grammar, syntax, and simple word associations, while the higher layers might capture more complex semantic relationships, discourse structures, and context-dependent meaning.

---

## Источники

1. [[Искусственный интеллект в стратегисческих играх]]
2. [LLM Visualization](https://bbycroft.net/llm)

## Ссылки

1. link
